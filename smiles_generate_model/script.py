# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫:

# pip install -r requirements.txt # –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª - –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫


#########################################################

# –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–∫–∏ + –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞):

#########################################################

import torch
import torch.nn as nn
from tokenizers import Tokenizer
from rdkit import Chem
import pandas as pd
import numpy as np

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –æ–±—É—á–µ–Ω–∏–µ–º!)
input_dim = 10
embed_dim = 128
hidden_dim = 256
vocab_size = 410  # –ó–∞–≥—Ä—É–∑–∏–º —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ–∑–∂–µ
max_len = 100

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # !!! –µ—Å–ª–∏ –Ω–µ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gpu, —Ç–æ –º–µ–Ω—è–µ–º –Ω–∞ device = torch.device("cpu") 

# --- –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ú–û–î–ï–õ–ò (—Ç–æ—á–Ω–æ –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏) ---
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.fc = nn.Linear(input_dim, hidden_dim)

    def forward(self, x):
        return torch.tanh(self.fc(x))

class Decoder(nn.Module):
    def __init__(self, embed_dim, hidden_dim, vocab_size, max_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.encoder_proj = nn.Linear(hidden_dim, hidden_dim)  # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è conditioning
        self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim, batch_first=True, dropout=0.1)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.max_len = max_len

    def forward(self, encoder_out, targets, teacher_forcing=True):
        batch_size, seq_len = targets.size()
        # –°—Ç–∞—Ä—Ç–æ–≤—ã–π input: embedding <start>
        inputs = self.embedding(targets[:, 0]).unsqueeze(1)  # batch x 1 x embed
        # Encoder output –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –ø–æ seq_len
        encoder_repeated = self.encoder_proj(encoder_out).unsqueeze(1).repeat(1, seq_len, 1)  # batch x seq x hidden
        outputs = []
        hidden = None
        for t in range(1, seq_len):  # –ù–∞—á–∏–Ω–∞–µ–º —Å t=1
            encoder_step = encoder_repeated[:, t, :].unsqueeze(1)  # batch x 1 x hidden
            combined = torch.cat([inputs, encoder_step], dim=-1)  # batch x 1 x (embed + hidden)
            out, hidden = self.lstm(combined, hidden)
            logit = self.fc(out.squeeze(1))  # batch x vocab
            outputs.append(logit)
            # Teacher forcing
            if teacher_forcing and t < seq_len:
                next_input = self.embedding(targets[:, t])
            else:
                next_input = self.embedding(torch.argmax(logit, dim=1))  # greedy
            inputs = next_input.unsqueeze(1)  # batch x 1 x embed
        return torch.stack(outputs, dim=1)  # batch x (seq-1) x vocab

class MoleculeGenerator(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_dim, vocab_size, max_len):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim)
        self.decoder = Decoder(embed_dim, hidden_dim, vocab_size, max_len)

    def forward(self, features, targets, teacher_forcing=True):
        encoder_out = self.encoder(features)
        return self.decoder(encoder_out, targets, teacher_forcing)

# --- –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò SMILES ---
def generate_smiles(model, features, tokenizer, max_len=100, temperature=1.0):
    model.eval()
    with torch.no_grad():
        features = features.unsqueeze(0).to(device) if features.dim() == 1 else features.to(device)  # batch=1
        encoder_out = model.encoder(features)  # 1 x hidden
        generated_ids = [tokenizer.token_to_id("<start>")]
        input_seq = torch.tensor([[tokenizer.token_to_id("<start>")]], dtype=torch.long).to(device)
        hidden = None
        for _ in range(max_len - 1):  # -1 –¥–ª—è start
            embedded = model.decoder.embedding(input_seq)  # 1 x 1 x embed
            encoder_proj = model.decoder.encoder_proj(encoder_out).unsqueeze(1)  # 1 x 1 x hidden
            combined = torch.cat([embedded, encoder_proj], dim=-1)  # 1 x 1 x (embed + hidden)
            out, hidden = model.decoder.lstm(combined, hidden)
            logit = model.decoder.fc(out.squeeze(1))  # 1 x vocab
            # Temperature-based sampling
            if temperature > 0:
                probs = torch.softmax(logit / temperature, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
            else:
                next_token = torch.argmax(logit, dim=1).item()
            generated_ids.append(next_token)
            if next_token == tokenizer.token_to_id("<end>"):
                break
            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)
        # Decode ids –≤ —Å—Ç—Ä–æ–∫—É: —É–±–∏—Ä–∞–µ–º <start> –∏ <end>, decode to SMILES
        generated_smiles_ids = generated_ids[1:]  # –£–±–∏—Ä–∞–µ–º <start>
        if generated_smiles_ids and generated_smiles_ids[-1] == tokenizer.token_to_id("<end>"):
            generated_smiles_ids = generated_smiles_ids[:-1]  # –£–±–∏—Ä–∞–µ–º <end>
        generated_smiles = tokenizer.decode(generated_smiles_ids, skip_special_tokens=True)  # BPE decode –≤ readable SMILES
        return generated_smiles

# --- –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ò –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê ---
# print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
tokenizer = Tokenizer.from_file("smiles_bpe.json")
actual_vocab_size = tokenizer.get_vocab_size()
# print(f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {actual_vocab_size}")

# print("–°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å...")
model = MoleculeGenerator(input_dim, embed_dim, hidden_dim, actual_vocab_size, max_len).to(device)

# print("–ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏...")
model.load_state_dict(torch.load("best_molecule_generator.pth", map_location=device, weights_only=True))
model.eval()

#########################################################

# –î–∞–Ω–Ω—ã–µ –Ω–∏–∂–µ –∑–∞–¥–∞—ë—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (+ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å –≤–≤–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö):

#########################################################

MW = 336.775
LogP = 2.29702
TPSA = 91.68
HBD = 3
HBA = 6
RB = 6
Atoms = 23
HeavyAtoms = 23
AromaticAtoms = 12
ChargedAtoms = 0


assert MW >= 18 and MW <= 1500, '–ü–∞—Ä–∞–º–µ—Ç—Ä MW –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ MW > 18 and MW < 1500'
assert LogP >= -10 and LogP <= 10, '–ü–∞—Ä–∞–º–µ—Ç—Ä LogP –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ LogP > -10 and LogP < 10'
assert TPSA >= 0 and TPSA <= 400, '–ü–∞—Ä–∞–º–µ—Ç—Ä TPSA –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ TPSA > 0 and TPSA < 400'
assert HBD >= 0 and HBD <= 15, '–ü–∞—Ä–∞–º–µ—Ç—Ä HBD –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ HBD > 0 and HBD < 15'
assert HBA >= 0 and HBA <= 25, '–ü–∞—Ä–∞–º–µ—Ç—Ä HBA –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ HBA > 0 and HBA < 25'
assert RB >= 0 and RB <= 50, '–ü–∞—Ä–∞–º–µ—Ç—Ä RB –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ RB > 0 and RB < 50'
assert Atoms >= 3 and Atoms <= 200, '–ü–∞—Ä–∞–º–µ—Ç—Ä Atoms –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ Atoms > 3 and Atoms < 200'
assert HeavyAtoms >= 1 and HeavyAtoms <= 150, '–ü–∞—Ä–∞–º–µ—Ç—Ä HeavyAtoms –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ HeavyAtoms > 1 and HeavyAtoms < 150'
assert AromaticAtoms >= 0 and AromaticAtoms <= 80, '–ü–∞—Ä–∞–º–µ—Ç—Ä AromaticAtoms –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ AromaticAtoms > 0 and AromaticAtoms < 80'
assert ChargedAtoms >= 0 and ChargedAtoms <= 20, '–ü–∞—Ä–∞–º–µ—Ç—Ä ChargedAtoms –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ChargedAtoms > 0 and ChargedAtoms < 20'

assert type(MW) == float or type(MW) == int , '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö MW –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º –∏–ª–∏ –¥—Ä–æ–±–Ω—ã–º —á–∏—Å–ª–æ–º.'
assert type(LogP) == float or type(LogP) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö LogP –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º –∏–ª–∏ –¥—Ä–æ–±–Ω—ã–º —á–∏—Å–ª–æ–º.'
assert type(TPSA) == float or type(TPSA) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö TPSA –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º –∏–ª–∏ –¥—Ä–æ–±–Ω—ã–º —á–∏—Å–ª–æ–º.'
assert type(HBD) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö HBD –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'
assert type(HBA) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö HBA –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'
assert type(RB) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö RB –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'
assert type(Atoms) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö Atoms –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'
assert type(HeavyAtoms) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö HeavyAtoms –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'
assert type(AromaticAtoms) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö AromaticAtoms –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'
assert type(ChargedAtoms) == int, '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö ChargedAtoms –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.'

#########################################################

# –î–∞–ª–µ–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è smiles –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:

#########################################################

from rdkit.Chem import Descriptors
import numpy as np
from sklearn.preprocessing import StandardScaler

N = 1000 # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –º–æ–ª–µ–∫—É–ª (–µ—Å–ª–∏ —Å—Ç–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ - –ø–æ–ª—É—á–∏–º –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –µ—Å–ª–∏ —Å—Ç–∞–≤–∏—Ç—å –º–µ–Ω—å—à–µ - —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –±—ã—Å—Ç—Ä–µ–µ)


features = [MW, LogP, TPSA, HBD, HBA, RB, Atoms, HeavyAtoms, AromaticAtoms, ChargedAtoms]
example_features = torch.tensor(features, dtype=torch.float32)

# üîá –û–¢–ö–õ–Æ–ß–ê–ï–ú –í–°–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø RDKit
from rdkit import RDLogger
RDLogger.DisableLog('rdApp.*')

# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –Ω–∞–¥—ë–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ ===
def validate_and_canonicalize(smiles):
    if not smiles or not isinstance(smiles, str):
        return None
    mol = Chem.MolFromSmiles(smiles, sanitize=False)
    if mol is None:
        return None
    try:
        Chem.SanitizeMol(mol, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL)
        return mol
    except:
        return None

# === –û–°–ù–û–í–ù–û–ô –ö–û–î ===
print("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SMILES...")

valid_entries = []

for i in range(N):
    smiles = generate_smiles(model, example_features, tokenizer, temperature=0.8)
    mol = validate_and_canonicalize(smiles)

    if mol is not None:
        props = np.array([
            Chem.Descriptors.MolWt(mol),
            Chem.Descriptors.MolLogP(mol),
            Chem.Descriptors.TPSA(mol),
            Chem.Descriptors.NumHDonors(mol),
            Chem.Descriptors.NumHAcceptors(mol),
            Chem.Descriptors.NumRotatableBonds(mol),
            mol.GetNumAtoms(),
            mol.GetNumHeavyAtoms(),
            sum(1 for a in mol.GetAtoms() if a.GetIsAromatic()),
            sum(1 for a in mol.GetAtoms() if a.GetFormalCharge() != 0)
        ], dtype=np.float64)
        valid_entries.append((smiles, mol, props))

if not valid_entries:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª–∏–¥–Ω—É—é –º–æ–ª–µ–∫—É–ª—É.")
else:
    all_props = np.array([entry[2] for entry in valid_entries])
    scaler = StandardScaler()
    all_props_scaled = scaler.fit_transform(all_props)

    user_props = np.array([
        MW, LogP, TPSA, HBD, HBA, RB,
        Atoms, HeavyAtoms, AromaticAtoms, ChargedAtoms
    ], dtype=np.float64).reshape(1, -1)
    user_props_scaled = scaler.transform(user_props)
    user_vec = user_props_scaled.flatten()

    distances = np.linalg.norm(all_props_scaled - user_vec, axis=1)
    best_idx = int(np.argmin(distances))
    best_smiles, best_mol, best_props = valid_entries[best_idx]
    canonical = Chem.MolToSmiles(best_mol, canonical=True)

    MW_gen, LogP_gen, TPSA_gen, HBD_gen, HBA_gen, RB_gen, \
    Atoms_gen, HeavyAtoms_gen, AromaticAtoms_gen, ChargedAtoms_gen = best_props

    def similarity_percent(gen, target):
        if abs(target) < 1e-8:
            return 100.0 if abs(gen) < 1e-8 else 0.0
        return max(0.0, 100.0 - 100.0 * abs(gen - target) / abs(target))

    similarities = [
        similarity_percent(MW_gen, MW),
        similarity_percent(LogP_gen, LogP),
        similarity_percent(TPSA_gen, TPSA),
        similarity_percent(HBD_gen, HBD),
        similarity_percent(HBA_gen, HBA),
        similarity_percent(RB_gen, RB),
        similarity_percent(Atoms_gen, Atoms),
        similarity_percent(HeavyAtoms_gen, HeavyAtoms),
        similarity_percent(AromaticAtoms_gen, AromaticAtoms),
        similarity_percent(ChargedAtoms_gen, ChargedAtoms)
    ]
    avg_similarity = np.mean(similarities)

    print("\n‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤–∞–ª–∏–¥–Ω–∞—è –º–æ–ª–µ–∫—É–ª–∞!")
    print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SMILES: {best_smiles}")
    print(f"–ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π SMILES: {canonical}")
    print(f"   Generated molecule properties: "
          f"MW={MW_gen:.2f}, LogP={LogP_gen:.2f}, TPSA={TPSA_gen:.2f}, "
          f"HBD={HBD_gen:.2f}, HBA={HBA_gen:.2f}, RB={RB_gen:.2f}, "
          f"Atoms={Atoms_gen:.2f}, HeavyAtoms={HeavyAtoms_gen:.2f}, "
          f"AromaticAtoms={AromaticAtoms_gen:.2f}, ChargedAtoms={ChargedAtoms_gen:.2f}")
    print(f"   Your molecule properties: "
          f"MW={MW:.2f}, LogP={LogP:.2f}, TPSA={TPSA:.2f}, "
          f"HBD={HBD:.2f}, HBA={HBA:.2f}, RB={RB:.2f}, "
          f"Atoms={Atoms:.2f}, HeavyAtoms={HeavyAtoms:.2f}, "
          f"AromaticAtoms={AromaticAtoms:.2f}, ChargedAtoms={ChargedAtoms:.2f}")
    print(f"   –°—Ö–æ–∂–µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {avg_similarity:.2f}%")